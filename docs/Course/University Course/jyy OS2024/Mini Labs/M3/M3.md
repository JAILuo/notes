## Introduction

由于开始的时间比较晚，在老师的 wiki 上无法下载 `gpt.c` 的 model 文件，所以网上找。







## Basic knowledge of AI

由于自己不像南大的学生可能在上 OS 课前就有 AI 相关的基础，所以这部分我根据老师推荐的学习资料和 LLM 来一步步学习相关概念，当然只是大概过一遍内容。

> 首先，`gpt.c` 的源代码就是最好的老师！它实现了对神经网络的 “真正数学严格” 的描述——如果我们看 PyTorch 的代码，其中会涉及许多内置的算子，你对其中的实现其实并不 100% 理解——但对于 C 代码这种语义 “扁平” 的语言来说，你真的可以完全理解它！此外，我们给一个[外链](https://jalammar.github.io/illustrated-gpt2/)，也还推荐 [Understanding Deep Learning](https://udlbook.github.io/udlbook/)。
>
> 当然，老师上面给出的一些学习资料也很不错！

加上之前偶然刷到的视频：[从函数到神经网络【白话DeepSeek01】](https://www.bilibili.com/video/BV1uGA3eLEeu) 和 [Neural network (machine learning)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))，应该能简单了解。最后再看看神经网络中的相关概念是怎么对应 C 代码实现。







### overview

####  一、神经元

神经元是神经网络的基本组成单元，它模拟了生物神经元的行为。一个简单的神经元模型由以下部分组成：

  * **输入连接** ：接收来自其他神经元或输入数据的信号。
  * **权重** ：每个输入连接都有一个权重，表示该输入对神经元输出的重要性。
  * **偏置** ：一个常数，用于调整神经元的激活阈值。
  * **激活函数** ：对神经元的加权输入进行非线性变换，以决定神经元是否被激活。

神经元的输出 y 的数学表达式：
$$
y = \sigma\left( \sum_{i=1}^{n} w_i x_i + b \right) \\

x_i 是输入，w_i 是权重，b 是偏置，\sigma 是激活函数
$$


#### 二、激活函数

激活函数是神经元中用于引入非线性的关键组件，它使得神经网络能够学习复杂的模式和特征。常见的激活函数有：

- **Sigmoid 函数**

      * **数学表达式** ：
        $$
        \sigma(x) = \frac{1}{1 + e^{-x}}
        $$

      * **特点** ：输出范围在 (0,1) 之间，可以将任意实数值映射到 0 和 1 之间，常用于二分类问题。然而，Sigmoid 函数在输入较大或较小时会出现梯度饱和问题，导致训练速度变慢。

      * **应用场景** ：适用于二分类任务的输出层，也可用于某些需要将输出限制在 0 到 1 之间的中间层。



- **`ReLU` 函数（Rectified Linear Unit）**

      * **数学表达式** ：
        $$
        \text{ReLU}(x) = \max(0, x)
        $$
        

      * **特点** ：当输入大于 0 时，输出等于输入；当输入小于等于 0 时，输出为 0。计算简单，能有效缓解梯度消失问题，但在输入小于 0 时梯度为 0，可能导致部分神经元 “死亡”。

      * **应用场景** ：广泛应用于卷积神经网络（CNN）和多层感知机（MLP）等的隐藏层。



- **tanh 函数**

      * **数学表达式** ：
        $$
        \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
        $$

      * **特点** ：输出范围在 (-1,1) 之间，是非线性函数，能够将输入数据归一化到 -1 到 1 的区间。相比 Sigmoid 函数，tanh 函数的输出均值更接近 0，有助于加快收敛速度，但也存在梯度饱和问题。

      * **应用场景** ：常用于循环神经网络（RNN）和某些需要将输出限制在 -1 到 1 之间的场景。



- **GELU 函数（Gaussian Error Linear Unit）**

      * **数学表达式** ：
        $$
        \text{GELU}(x) = x \cdot \Phi(x)，\\
        \Phi(x) 是标准正态分布的累积分布函数。
        $$
        

      * **特点** ：结合了 `ReLU` 和线性函数的优点，具有更好的非线性拟合能力，在 Transformer 模型中被广泛使用。

      * **应用场景** ：适用于 Transformer 架构的神经网络，特别是在自然语言处理任务中。



- `Softmax` 函数

      * **数学表达式** ：

        对于输入向量：
        $$
        z = [z_1, z_2, ..., z_n]
        $$
        `Softmax` 函数的输出为：
        $$
        \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \\
        (i = 1, 2, ..., n)
        $$
        

      * **特点** ：将一组实数值转换为概率分布，使得所有输出值的和为 1，常用于多分类问题的输出层。

      * **应用场景** ：主要用于多分类任务的输出层，将神经网络的输出转换为各类别的概率。



- **`Leaky ReLU` 函数**

      * **数学表达式** ：
        $$
        \text{Leaky ReLU}(x) = \begin{cases} x, & x > 0 \\ \alpha x, & x \leq 0 \end{cases} \\
        \alpha 是一个较小的正数，如 0.01
        $$
        

      * **特点** ：在输入小于 0 时，输出为 αx ，避免了 `ReLU` 函数中部分神经元 “死亡” 的问题，同时保留了 `ReLU` 函数在输入大于 0 时的线性特性。

      * **应用场景** ：适用于需要解决 `ReLU` 函数 “死亡” 问题的神经网络隐藏层。



- **ELU 函数（Exponential Linear Unit）**

      * **数学表达式**：
        $$
        \text{ELU}(x) = \begin{cases} x, & x > 0 \\ \alpha (e^{x} - 1), & x \leq 0 \end{cases} \\
        \alpha 是一个正数
        $$

      * **特点** ：在输入小于 0 时，输出为 \(\alpha (e^{x} - 1)\)，能够平滑地过渡到负值，有助于缓解梯度消失问题，同时避免了神经元 “死亡” 现象。

      * **应用场景** ：适用于需要更好收敛性能和避免神经元 “死亡” 的神经网络隐藏层。





####  三、神经网络的结构

一个典型的神经网络由以下几层组成：

1. 输入层

    负责接收输入数据，其神经元数量通常与输入特征的维度一致。

2. 隐藏层

    位于输入层和输出层之间的层，可以有多个隐藏层，每个隐藏层包含一定数量的神经元。隐藏层通过非线性激活函数对输入数据进行特征提取和变换。

3. 输出层

    产生神经网络的最终输出结果，其神经元数量取决于任务的类型，例如在分类任务中，输出层神经元数量通常与类别数一致。

直接配图，当然实际情况更加复杂！

![image-20250311003721235](pic/image-20250311003721235.png)





#### 四、矩阵运算在神经网络中的应用

在神经网络中，矩阵运算是高效实现神经元计算的关键。

例如，对于一个包含多个神经元的层，我们可以将输入数据、权重和偏置表示为矩阵和向量，然后通过矩阵乘法和加法运算快速计算出该层所有神经元的输出。

假设输入数据为一个 \(m \times n\) 的矩阵 \(X\)（表示有 \(m\) 个样本，每个样本有 \(n\) 个特征），权重矩阵为一个 \(n \times p\) 的矩阵 \(W\)（表示从输入层到隐藏层的连接权重，隐藏层有 \(p\) 个神经元），偏置向量为一个 \(1 \times p\) 的向量 \(b\)，则隐藏层的输出矩阵 \(H\) 可以通过以下矩阵运算得到：

$$
H = \sigma(X \cdot W + b) \\
\sigma 表示逐元素应用激活函数。
$$


#### 五、层归一化（Layer Normalization）

层归一化是一种对神经网络中间层的输出进行归一化的技术，旨在稳定网络的训练过程和提高收敛速度。它通过对每个样本在每个神经元上的输出进行归一化，使得这些输出具有零均值和单位方差，然后通过学习的参数进行缩放和平移操作。

对于输入数据 \(x\)，层归一化的输出 y 数学表达计算如下：
$$
y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \\
\mu 和\sigma^2 分别是输入数据在该层神经元上的均值和方差 \\
\gamma 和 \beta 是学习的缩放和平移参数 \\
\epsilon 是为了避免除零的微小常数。
$$




#### 六、注意力机制（Attention）

注意力机制是现代神经网络，特别是 Transformer 模型中的核心组件之一。它允许模型在处理序列数据时，能够动态地关注序列中的不同位置，从而更好地捕捉序列中的长期依赖关系。

- 自注意力（Self-Attention）

自注意力机制通过计算序列中每个位置的查询向量（Query）、键向量（Key）和值向量（Value）之间的相似度，来确定每个位置应该关注的其他位置的程度。具体步骤如下：

  1. **生成 Q、K、V 向量** ：对输入序列中的每个元素，通过线性变换生成查询向量 \(Q\)、键向量 \(K\) 和值向量 \(V\)。
  2. **计算注意力分数** ：通过 \(Q\) 和 \(K\) 的点积计算注意力分数，表示每个位置之间的相关性。
  3. **缩放和 `softmax` 操作** ：将注意力分数除以键向量维度的平方根进行缩放，然后应用 `softmax` 函数得到注意力权重，这些权重表示每个位置应该分配多少关注给其他位置。
  4. **加权求和** ：将注意力权重与对应的值向量进行加权求和，得到最终的输出。



#### 七、软件最大值函数（`Softmax`）

`Softmax` 函数常用于神经网络的输出层，特别是在多分类问题中。它将一组实数值转换为概率分布，使得所有输出值的和为 1。

对于输入向量
$$
z = [z_1, z_2, ..., z_n]
$$
`Softmax` 函数的输出为：
$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\\

(i = 1, 2, ..., n)
$$


#### 八、残差连接（Residual Connection）

残差连接是一种网络结构设计，它通过将输入直接加到输出上，形成一个残差块。这种设计有助于缓解深度神经网络中的梯度消失问题，使得网络能够更容易地训练更深的层次。

残差块的输出 \(y\) 的数学表达：

$$
y = F(x) + x\\
x 是输入，F(x) 是残差块中的主路径网络结构的输出。
$$




#### 九、Transformer 模型概述

Transformer 模型是一种基于自注意力机制的神经网络架构，广泛应用于自然语言处理任务中，如机器翻译、文本生成等。它主要由以下组件构成：

  * **编码器（Encoder）** ：对输入序列进行编码，提取特征表示。
  * **解码器（Decoder）** ：根据编码器的输出和之前的解码结果生成输出序列。
  * **自注意力层（Self-Attention Layer）** ：在编码器和解码器中都使用，用于捕捉序列中的依赖关系。
  * **前馈神经网络（Feed-Forward Neural Network）** ：对每个位置的特征进行非线性变换。



#### 十、GPT-2 模型简介

GPT-2 是一种基于 Transformer 架构的预训练语言模型，它通过在大规模文本数据上进行无监督预训练，学习到了丰富的语言知识和语义信息。GPT-2 主要由多个 Transformer 解码器层堆叠而成，每个解码器层包含自注意力机制和前馈神经网络。在实际应用中，可以通过微调 GPT-2 模型来适应特定的自然语言处理任务，如文本生成、问答系统等。















## Usage of Tools
